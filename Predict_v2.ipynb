{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import types\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pyaudio\n",
    "import wave\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "import sys\n",
    "sys.path.insert(1, 'H:\\\\IT_DUT\\\\Ki6\\\\PBL5\\\\Code_Test1\\\\Speech-Recognition-with-RNN-Neural-Networks\\\\utils')\n",
    "import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = os.path.abspath(os.getcwd())\n",
    "DATASET_DIRECTORY_PATH = DIR+'/Data_Test/Thanh_clean'\n",
    "train_audio_path =DATASET_DIRECTORY_PATH+\"/\"\n",
    "labels = os.listdir(train_audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model, weights, PRINT=False):\n",
    "    # Load weights into model.\n",
    "    # If param's name is different, raise error.\n",
    "    # If param's size is different, skip this param.\n",
    "    # see: https://discuss.pytorch.org/t/how-to-load-part-of-pre-trained-model/1113/2\n",
    "    \n",
    "    for i, (name, param) in enumerate(weights.items()):\n",
    "        model_state = model.state_dict()\n",
    "        \n",
    "        if name not in model_state:\n",
    "            print(\"-\"*80)\n",
    "            print(\"weights name:\", name) \n",
    "            print(\"RNN states names:\", model_state.keys()) \n",
    "            assert 0, \"Wrong weights file\"\n",
    "            \n",
    "        model_shape = model_state[name].shape\n",
    "        if model_shape != param.shape:\n",
    "            print(f\"\\nWarning: Size of {name} layer is different between model and weights. Not copy parameters.\")\n",
    "            print(f\"\\tModel shape = {model_shape}, weights' shape = {param.shape}.\")\n",
    "        else:\n",
    "            model_state[name].copy_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_default_args():\n",
    "    \n",
    "    args = types.SimpleNamespace()\n",
    "\n",
    "    # model params\n",
    "    args.input_size = 12  # == n_mfcc\n",
    "    args.batch_size = 1\n",
    "    args.hidden_size = 64\n",
    "    args.num_layers = 3\n",
    "\n",
    "    # training params\n",
    "    args.num_epochs = 100\n",
    "    args.learning_rate = 0.0001\n",
    "    args.learning_rate_decay_interval = 5 # decay for every 5 epochs\n",
    "    args.learning_rate_decay_rate = 0.5 # lr = lr * rate\n",
    "    args.weight_decay = 0.00\n",
    "    args.gradient_accumulations = 16 # number of gradient accums before step\n",
    "    \n",
    "    # training params2\n",
    "    args.load_weights_from = None\n",
    "    args.finetune_model = False # If true, fix all parameters except the fc layer\n",
    "    args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # data\n",
    "    args.data_folder = \"Data_Test/data_train/\"\n",
    "    args.train_eval_test_ratio=[0.0, 0.0, 1.0]\n",
    "    args.do_data_augment = False\n",
    "\n",
    "    # labels\n",
    "    #args.classes_txt = \"config/classes.names\" \n",
    "    args.classes_txt = labels\n",
    "    args.num_classes = None # should be added with a value somewhere, like this:\n",
    "    #                = len(lib.read_list(args.classes_txt))\n",
    "\n",
    "    # log setting\n",
    "    args.plot_accu = True # if true, plot accuracy for every epoch\n",
    "    args.show_plotted_accu = False # if false, not calling plt.show(), so drawing figure in background\n",
    "    args.save_model_to = 'checkpoints/' # Save model and log file\n",
    "        #e.g: model_001.ckpt, log.txt, log.jpg\n",
    "    \n",
    "    return args "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN_model(args, load_weights_from=None):\n",
    "    ''' A wrapper for creating a 'class RNN' instance '''\n",
    "    # Update some dependent args\n",
    "    #args.num_classes = len(lib.read_list(args.classes_txt)) # read from \"config/classes.names\"\n",
    "    args.num_classes = len(labels) # read from \"config/classes.names\"\n",
    "    args.save_log_to = args.save_model_to + \"log.txt\"\n",
    "    args.save_fig_to = args.save_model_to + \"fig.jpg\"\n",
    "    \n",
    "    # Create model\n",
    "    device = args.device\n",
    "    model = RNN(args.input_size, args.hidden_size, args.num_layers, args.num_classes, device).to(device)\n",
    "    \n",
    "    # Load weights\n",
    "    if load_weights_from:\n",
    "        print(f\"Load weights from: {load_weights_from}\")\n",
    "        weights = torch.load(load_weights_from)\n",
    "        load_weights(model, weights)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent neural network (many-to-one)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, device, classes=None):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.device = device\n",
    "        self.classes = classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(self.device) \n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(self.device) \n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # shape = (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def predict(self, x):\n",
    "        '''Predict one label from one sample's features'''\n",
    "        # x: feature from a sample, LxN\n",
    "        #   L is length of sequency\n",
    "        #   N is feature dimension\n",
    "        x = torch.tensor(x[np.newaxis, :], dtype=torch.float32)\n",
    "        x = x.to(self.device)\n",
    "        outputs = self.forward(x)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted_index = predicted.item()\n",
    "        return predicted_index\n",
    "    \n",
    "    def set_classes(self, classes):\n",
    "        self.classes = classes \n",
    "    \n",
    "    def predict_audio_label(self, audio):\n",
    "        idx = self.predict_audio_label_index(audio)\n",
    "        assert self.classes, \"Classes names are not set. Don't know what audio label is\"\n",
    "        label = self.classes[idx]\n",
    "        return label\n",
    "\n",
    "    def predict_audio_label_index(self, audio):\n",
    "        audio.compute_mfcc()\n",
    "        x = audio.mfcc.T # (time_len, feature_dimension)\n",
    "        idx = self.predict(x)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_classifier(load_weights_from):\n",
    "    model_args = set_default_args()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = create_RNN_model(model_args, load_weights_from)\n",
    "    return model\n",
    "\n",
    "def setup_classes_labels(load_classes_from, model):\n",
    "    classes = lib.read_list(load_classes_from)\n",
    "    print(f\"{len(classes)} classes: {classes}\")\n",
    "    model.set_classes(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weights from: checkpoints//025.ckpt\n",
      "8 classes: ['BatDen', 'BatDenLen', 'SangQua', 'TatDen', 'TatDenDi', 'ToiQua', 'XoayGhePhai', 'XoayGheTrai']\n"
     ]
    }
   ],
   "source": [
    "model = setup_classifier(load_weights_from=\"checkpoints//025.ckpt\")\n",
    "setup_classes_labels(load_classes_from=\"config/classes.names\", model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start recording\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Left: 3 seconds\n",
      "Time Left: 2 seconds\n",
      "Time Left: 1 seconds\n",
      "Time Left: 0 seconds\n"
     ]
    }
   ],
   "source": [
    "FRAMES_PER_BUFFER = 3200\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "\n",
    "pa = pyaudio.PyAudio()\n",
    "\n",
    "stream = pa.open(\n",
    "    format=FORMAT,\n",
    "    channels=CHANNELS,\n",
    "    rate=RATE,\n",
    "    input=True,\n",
    "    frames_per_buffer=FRAMES_PER_BUFFER\n",
    ")\n",
    "\n",
    "print('start recording')\n",
    "\n",
    "seconds = 4\n",
    "frames = []\n",
    "second_tracking = 0\n",
    "second_count = 0\n",
    "for i in range(0, int(RATE/FRAMES_PER_BUFFER*seconds)):\n",
    "    data = stream.read(FRAMES_PER_BUFFER)\n",
    "    frames.append(data)\n",
    "    second_tracking += 1\n",
    "    if second_tracking == RATE/FRAMES_PER_BUFFER:\n",
    "        second_count += 1\n",
    "        second_tracking = 0\n",
    "        print(f'Time Left: {seconds - second_count} seconds')\n",
    "\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "pa.terminate()\n",
    "\n",
    "obj = wave.open('output.wav', 'wb')\n",
    "obj.setnchannels(CHANNELS)\n",
    "obj.setsampwidth(pa.get_sample_size(FORMAT))\n",
    "obj.setframerate(RATE)\n",
    "obj.writeframes(b''.join(frames))\n",
    "obj.close()\n",
    "\n",
    "\n",
    "file = wave.open('output.wav', 'rb')\n",
    "\n",
    "sample_freq = file.getframerate()\n",
    "frames = file.getnframes()\n",
    "signal_wave = file.readframes(-1)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatDen\n"
     ]
    }
   ],
   "source": [
    "audio = lib.AudioClass(filename='H:\\\\IT_DUT\\\\Ki6\\\\PBL5\\\\Code_Test1\\\\Speech-Recognition-with-RNN-Neural-Networks\\\\output.wav')\n",
    "prediction=model.predict_audio_label(audio)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
